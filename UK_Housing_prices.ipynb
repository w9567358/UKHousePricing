{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.04 s, sys: 581 ms, total: 2.62 s\n",
      "Wall time: 1.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels\n",
    "import numpy as np \n",
    "import matplotlib.gridspec as gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.36 s, sys: 258 ms, total: 2.62 s\n",
      "Wall time: 2.62 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = pd.read_csv(\"pp-partial-complete.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>{F887F88E-7D15-4415-804E-52EAC2F10958}</th>\n",
       "      <th>70000</th>\n",
       "      <th>07/07/95 0:00</th>\n",
       "      <th>MK15 9HP</th>\n",
       "      <th>D</th>\n",
       "      <th>N</th>\n",
       "      <th>F</th>\n",
       "      <th>31</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "      <th>ALDRICH DRIVE</th>\n",
       "      <th>WILLEN</th>\n",
       "      <th>MILTON KEYNES</th>\n",
       "      <th>MILTON KEYNES.1</th>\n",
       "      <th>MILTON KEYNES.2</th>\n",
       "      <th>A</th>\n",
       "      <th>A.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{40FD4DF2-5362-407C-92BC-566E2CCE89E9}</td>\n",
       "      <td>44500</td>\n",
       "      <td>03/02/95 0:00</td>\n",
       "      <td>SR6 0AQ</td>\n",
       "      <td>T</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HOWICK PARK</td>\n",
       "      <td>SUNDERLAND</td>\n",
       "      <td>SUNDERLAND</td>\n",
       "      <td>SUNDERLAND</td>\n",
       "      <td>TYNE AND WEAR</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{7A99F89E-7D81-4E45-ABD5-566E49A045EA}</td>\n",
       "      <td>56500</td>\n",
       "      <td>13/01/95 0:00</td>\n",
       "      <td>CO6 1SQ</td>\n",
       "      <td>T</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BRICK KILN CLOSE</td>\n",
       "      <td>COGGESHALL</td>\n",
       "      <td>COLCHESTER</td>\n",
       "      <td>BRAINTREE</td>\n",
       "      <td>ESSEX</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{28225260-E61C-4E57-8B56-566E5285B1C1}</td>\n",
       "      <td>58000</td>\n",
       "      <td>28/07/95 0:00</td>\n",
       "      <td>B90 4TG</td>\n",
       "      <td>T</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>37</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RAINSBROOK DRIVE</td>\n",
       "      <td>SHIRLEY</td>\n",
       "      <td>SOLIHULL</td>\n",
       "      <td>SOLIHULL</td>\n",
       "      <td>WEST MIDLANDS</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{444D34D7-9BA6-43A7-B695-4F48980E0176}</td>\n",
       "      <td>51000</td>\n",
       "      <td>28/06/95 0:00</td>\n",
       "      <td>DY5 1SA</td>\n",
       "      <td>S</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MERRY HILL</td>\n",
       "      <td>BRIERLEY HILL</td>\n",
       "      <td>BRIERLEY HILL</td>\n",
       "      <td>DUDLEY</td>\n",
       "      <td>WEST MIDLANDS</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{AE76CAF1-F8CC-43F9-8F63-4F48A2857D41}</td>\n",
       "      <td>17000</td>\n",
       "      <td>10/03/95 0:00</td>\n",
       "      <td>S65 1QJ</td>\n",
       "      <td>T</td>\n",
       "      <td>N</td>\n",
       "      <td>L</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DENMAN STREET</td>\n",
       "      <td>ROTHERHAM</td>\n",
       "      <td>ROTHERHAM</td>\n",
       "      <td>ROTHERHAM</td>\n",
       "      <td>SOUTH YORKSHIRE</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004826</th>\n",
       "      <td>{EC1AD316-A622-420D-AB2A-67F1110B2549}</td>\n",
       "      <td>67000</td>\n",
       "      <td>08/03/96 0:00</td>\n",
       "      <td>MK41 8AP</td>\n",
       "      <td>S</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FOXLEASE</td>\n",
       "      <td>BEDFORD</td>\n",
       "      <td>BEDFORD</td>\n",
       "      <td>BEDFORD</td>\n",
       "      <td>BEDFORDSHIRE</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004827</th>\n",
       "      <td>{4D92D63F-9A76-4644-94D0-67F1152C92A5}</td>\n",
       "      <td>177500</td>\n",
       "      <td>28/06/96 0:00</td>\n",
       "      <td>SW19 3AD</td>\n",
       "      <td>T</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>QUEENSLAND AVENUE</td>\n",
       "      <td>LONDON</td>\n",
       "      <td>LONDON</td>\n",
       "      <td>MERTON</td>\n",
       "      <td>GREATER LONDON</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004828</th>\n",
       "      <td>{170CE10A-899F-4A82-9C82-67F12FF36E5F}</td>\n",
       "      <td>23000</td>\n",
       "      <td>19/09/96 0:00</td>\n",
       "      <td>M14 5SA</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>L</td>\n",
       "      <td>MARTIN HOUSE, 4</td>\n",
       "      <td>FLAT 5</td>\n",
       "      <td>CONYNGHAM ROAD</td>\n",
       "      <td>MANCHESTER</td>\n",
       "      <td>MANCHESTER</td>\n",
       "      <td>MANCHESTER</td>\n",
       "      <td>GREATER MANCHESTER</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004829</th>\n",
       "      <td>{0B83F6EA-EB26-4807-8BBA-67F141F30EBA}</td>\n",
       "      <td>36000</td>\n",
       "      <td>07/06/96 0:00</td>\n",
       "      <td>CB9 9EN</td>\n",
       "      <td>S</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BRAMBLE CLOSE</td>\n",
       "      <td>HAVERHILL</td>\n",
       "      <td>HAVERHILL</td>\n",
       "      <td>ST EDMUNDSBURY</td>\n",
       "      <td>SUFFOLK</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004830</th>\n",
       "      <td>{950095C7-DFA7-48A1-907F-67F142BE035D}</td>\n",
       "      <td>26350</td>\n",
       "      <td>28/06/96 0:00</td>\n",
       "      <td>HD4 7LZ</td>\n",
       "      <td>T</td>\n",
       "      <td>N</td>\n",
       "      <td>L</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1004831 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         {F887F88E-7D15-4415-804E-52EAC2F10958}   70000  07/07/95 0:00  \\\n",
       "0        {40FD4DF2-5362-407C-92BC-566E2CCE89E9}   44500  03/02/95 0:00   \n",
       "1        {7A99F89E-7D81-4E45-ABD5-566E49A045EA}   56500  13/01/95 0:00   \n",
       "2        {28225260-E61C-4E57-8B56-566E5285B1C1}   58000  28/07/95 0:00   \n",
       "3        {444D34D7-9BA6-43A7-B695-4F48980E0176}   51000  28/06/95 0:00   \n",
       "4        {AE76CAF1-F8CC-43F9-8F63-4F48A2857D41}   17000  10/03/95 0:00   \n",
       "...                                         ...     ...            ...   \n",
       "1004826  {EC1AD316-A622-420D-AB2A-67F1110B2549}   67000  08/03/96 0:00   \n",
       "1004827  {4D92D63F-9A76-4644-94D0-67F1152C92A5}  177500  28/06/96 0:00   \n",
       "1004828  {170CE10A-899F-4A82-9C82-67F12FF36E5F}   23000  19/09/96 0:00   \n",
       "1004829  {0B83F6EA-EB26-4807-8BBA-67F141F30EBA}   36000  07/06/96 0:00   \n",
       "1004830  {950095C7-DFA7-48A1-907F-67F142BE035D}   26350  28/06/96 0:00   \n",
       "\n",
       "         MK15 9HP  D  N  F               31 Unnamed: 8      ALDRICH DRIVE  \\\n",
       "0         SR6 0AQ  T  N  F               50        NaN        HOWICK PARK   \n",
       "1         CO6 1SQ  T  N  F               19        NaN   BRICK KILN CLOSE   \n",
       "2         B90 4TG  T  N  F               37        NaN   RAINSBROOK DRIVE   \n",
       "3         DY5 1SA  S  N  F               59        NaN         MERRY HILL   \n",
       "4         S65 1QJ  T  N  L               22        NaN      DENMAN STREET   \n",
       "...           ... .. .. ..              ...        ...                ...   \n",
       "1004826  MK41 8AP  S  N  F               36        NaN           FOXLEASE   \n",
       "1004827  SW19 3AD  T  N  F               19        NaN  QUEENSLAND AVENUE   \n",
       "1004828   M14 5SA  F  N  L  MARTIN HOUSE, 4     FLAT 5     CONYNGHAM ROAD   \n",
       "1004829   CB9 9EN  S  N  F                5        NaN      BRAMBLE CLOSE   \n",
       "1004830   HD4 7LZ  T  N  L                4        NaN                NaN   \n",
       "\n",
       "                WILLEN  MILTON KEYNES MILTON KEYNES.1     MILTON KEYNES.2  \\\n",
       "0           SUNDERLAND     SUNDERLAND      SUNDERLAND       TYNE AND WEAR   \n",
       "1           COGGESHALL     COLCHESTER       BRAINTREE               ESSEX   \n",
       "2              SHIRLEY       SOLIHULL        SOLIHULL       WEST MIDLANDS   \n",
       "3        BRIERLEY HILL  BRIERLEY HILL          DUDLEY       WEST MIDLANDS   \n",
       "4            ROTHERHAM      ROTHERHAM       ROTHERHAM     SOUTH YORKSHIRE   \n",
       "...                ...            ...             ...                 ...   \n",
       "1004826        BEDFORD        BEDFORD         BEDFORD        BEDFORDSHIRE   \n",
       "1004827         LONDON         LONDON          MERTON      GREATER LONDON   \n",
       "1004828     MANCHESTER     MANCHESTER      MANCHESTER  GREATER MANCHESTER   \n",
       "1004829      HAVERHILL      HAVERHILL  ST EDMUNDSBURY             SUFFOLK   \n",
       "1004830            NaN            NaN             NaN                 NaN   \n",
       "\n",
       "           A  A.1  \n",
       "0          A    A  \n",
       "1          A    A  \n",
       "2          A    A  \n",
       "3          A    A  \n",
       "4          A    A  \n",
       "...      ...  ...  \n",
       "1004826    A    A  \n",
       "1004827    A    A  \n",
       "1004828    A    A  \n",
       "1004829    A    A  \n",
       "1004830  NaN  NaN  \n",
       "\n",
       "[1004831 rows x 16 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns = {\"{5BBE9CB3-6332-4EB0-9CD3-8737CEA4A65A}\": 'Transaction unique identifier',\n",
    "                    \"42000\": \"Price\",\"1995-12-21 00:00\":\"Date of Transfer\",\"NE4 9DN\": \"Postcode\",\"S\":\"Property Type\",\"N\":\"Old/New\",\"F\":\"Duration\",\n",
    "                    \"8\":\"PAON\",\"Unnamed: 8\":\"SAON\",\"MATFEN PLACE\":\"Street\",\"FENHAM\":\"Locality\",\"NEWCASTLE UPON TYNE\":\"Town/City\",\n",
    "                    \"NEWCASTLE UPON TYNE.1\":\"District\",\"TYNE AND WEAR\":\"County\",\"A\":\"PPD Category Type\",\"A.1\":\"Record Status - monthly file only\"},inplace = True)\n",
    "\n",
    "# Its ok to rename this column thats acutally part of the dataframe since this row is not part of South East England"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>{F887F88E-7D15-4415-804E-52EAC2F10958}</th>\n",
       "      <th>70000</th>\n",
       "      <th>07/07/95 0:00</th>\n",
       "      <th>MK15 9HP</th>\n",
       "      <th>D</th>\n",
       "      <th>Old/New</th>\n",
       "      <th>Duration</th>\n",
       "      <th>31</th>\n",
       "      <th>SAON</th>\n",
       "      <th>ALDRICH DRIVE</th>\n",
       "      <th>WILLEN</th>\n",
       "      <th>MILTON KEYNES</th>\n",
       "      <th>MILTON KEYNES.1</th>\n",
       "      <th>MILTON KEYNES.2</th>\n",
       "      <th>PPD Category Type</th>\n",
       "      <th>Record Status - monthly file only</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{40FD4DF2-5362-407C-92BC-566E2CCE89E9}</td>\n",
       "      <td>44500</td>\n",
       "      <td>03/02/95 0:00</td>\n",
       "      <td>SR6 0AQ</td>\n",
       "      <td>T</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HOWICK PARK</td>\n",
       "      <td>SUNDERLAND</td>\n",
       "      <td>SUNDERLAND</td>\n",
       "      <td>SUNDERLAND</td>\n",
       "      <td>TYNE AND WEAR</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{7A99F89E-7D81-4E45-ABD5-566E49A045EA}</td>\n",
       "      <td>56500</td>\n",
       "      <td>13/01/95 0:00</td>\n",
       "      <td>CO6 1SQ</td>\n",
       "      <td>T</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BRICK KILN CLOSE</td>\n",
       "      <td>COGGESHALL</td>\n",
       "      <td>COLCHESTER</td>\n",
       "      <td>BRAINTREE</td>\n",
       "      <td>ESSEX</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{28225260-E61C-4E57-8B56-566E5285B1C1}</td>\n",
       "      <td>58000</td>\n",
       "      <td>28/07/95 0:00</td>\n",
       "      <td>B90 4TG</td>\n",
       "      <td>T</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>37</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RAINSBROOK DRIVE</td>\n",
       "      <td>SHIRLEY</td>\n",
       "      <td>SOLIHULL</td>\n",
       "      <td>SOLIHULL</td>\n",
       "      <td>WEST MIDLANDS</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{444D34D7-9BA6-43A7-B695-4F48980E0176}</td>\n",
       "      <td>51000</td>\n",
       "      <td>28/06/95 0:00</td>\n",
       "      <td>DY5 1SA</td>\n",
       "      <td>S</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MERRY HILL</td>\n",
       "      <td>BRIERLEY HILL</td>\n",
       "      <td>BRIERLEY HILL</td>\n",
       "      <td>DUDLEY</td>\n",
       "      <td>WEST MIDLANDS</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{AE76CAF1-F8CC-43F9-8F63-4F48A2857D41}</td>\n",
       "      <td>17000</td>\n",
       "      <td>10/03/95 0:00</td>\n",
       "      <td>S65 1QJ</td>\n",
       "      <td>T</td>\n",
       "      <td>N</td>\n",
       "      <td>L</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DENMAN STREET</td>\n",
       "      <td>ROTHERHAM</td>\n",
       "      <td>ROTHERHAM</td>\n",
       "      <td>ROTHERHAM</td>\n",
       "      <td>SOUTH YORKSHIRE</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004826</th>\n",
       "      <td>{EC1AD316-A622-420D-AB2A-67F1110B2549}</td>\n",
       "      <td>67000</td>\n",
       "      <td>08/03/96 0:00</td>\n",
       "      <td>MK41 8AP</td>\n",
       "      <td>S</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FOXLEASE</td>\n",
       "      <td>BEDFORD</td>\n",
       "      <td>BEDFORD</td>\n",
       "      <td>BEDFORD</td>\n",
       "      <td>BEDFORDSHIRE</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004827</th>\n",
       "      <td>{4D92D63F-9A76-4644-94D0-67F1152C92A5}</td>\n",
       "      <td>177500</td>\n",
       "      <td>28/06/96 0:00</td>\n",
       "      <td>SW19 3AD</td>\n",
       "      <td>T</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>QUEENSLAND AVENUE</td>\n",
       "      <td>LONDON</td>\n",
       "      <td>LONDON</td>\n",
       "      <td>MERTON</td>\n",
       "      <td>GREATER LONDON</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004828</th>\n",
       "      <td>{170CE10A-899F-4A82-9C82-67F12FF36E5F}</td>\n",
       "      <td>23000</td>\n",
       "      <td>19/09/96 0:00</td>\n",
       "      <td>M14 5SA</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>L</td>\n",
       "      <td>MARTIN HOUSE, 4</td>\n",
       "      <td>FLAT 5</td>\n",
       "      <td>CONYNGHAM ROAD</td>\n",
       "      <td>MANCHESTER</td>\n",
       "      <td>MANCHESTER</td>\n",
       "      <td>MANCHESTER</td>\n",
       "      <td>GREATER MANCHESTER</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004829</th>\n",
       "      <td>{0B83F6EA-EB26-4807-8BBA-67F141F30EBA}</td>\n",
       "      <td>36000</td>\n",
       "      <td>07/06/96 0:00</td>\n",
       "      <td>CB9 9EN</td>\n",
       "      <td>S</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BRAMBLE CLOSE</td>\n",
       "      <td>HAVERHILL</td>\n",
       "      <td>HAVERHILL</td>\n",
       "      <td>ST EDMUNDSBURY</td>\n",
       "      <td>SUFFOLK</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004830</th>\n",
       "      <td>{950095C7-DFA7-48A1-907F-67F142BE035D}</td>\n",
       "      <td>26350</td>\n",
       "      <td>28/06/96 0:00</td>\n",
       "      <td>HD4 7LZ</td>\n",
       "      <td>T</td>\n",
       "      <td>N</td>\n",
       "      <td>L</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1004831 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         {F887F88E-7D15-4415-804E-52EAC2F10958}   70000  07/07/95 0:00  \\\n",
       "0        {40FD4DF2-5362-407C-92BC-566E2CCE89E9}   44500  03/02/95 0:00   \n",
       "1        {7A99F89E-7D81-4E45-ABD5-566E49A045EA}   56500  13/01/95 0:00   \n",
       "2        {28225260-E61C-4E57-8B56-566E5285B1C1}   58000  28/07/95 0:00   \n",
       "3        {444D34D7-9BA6-43A7-B695-4F48980E0176}   51000  28/06/95 0:00   \n",
       "4        {AE76CAF1-F8CC-43F9-8F63-4F48A2857D41}   17000  10/03/95 0:00   \n",
       "...                                         ...     ...            ...   \n",
       "1004826  {EC1AD316-A622-420D-AB2A-67F1110B2549}   67000  08/03/96 0:00   \n",
       "1004827  {4D92D63F-9A76-4644-94D0-67F1152C92A5}  177500  28/06/96 0:00   \n",
       "1004828  {170CE10A-899F-4A82-9C82-67F12FF36E5F}   23000  19/09/96 0:00   \n",
       "1004829  {0B83F6EA-EB26-4807-8BBA-67F141F30EBA}   36000  07/06/96 0:00   \n",
       "1004830  {950095C7-DFA7-48A1-907F-67F142BE035D}   26350  28/06/96 0:00   \n",
       "\n",
       "         MK15 9HP  D Old/New Duration               31    SAON  \\\n",
       "0         SR6 0AQ  T       N        F               50     NaN   \n",
       "1         CO6 1SQ  T       N        F               19     NaN   \n",
       "2         B90 4TG  T       N        F               37     NaN   \n",
       "3         DY5 1SA  S       N        F               59     NaN   \n",
       "4         S65 1QJ  T       N        L               22     NaN   \n",
       "...           ... ..     ...      ...              ...     ...   \n",
       "1004826  MK41 8AP  S       N        F               36     NaN   \n",
       "1004827  SW19 3AD  T       N        F               19     NaN   \n",
       "1004828   M14 5SA  F       N        L  MARTIN HOUSE, 4  FLAT 5   \n",
       "1004829   CB9 9EN  S       N        F                5     NaN   \n",
       "1004830   HD4 7LZ  T       N        L                4     NaN   \n",
       "\n",
       "             ALDRICH DRIVE         WILLEN  MILTON KEYNES MILTON KEYNES.1  \\\n",
       "0              HOWICK PARK     SUNDERLAND     SUNDERLAND      SUNDERLAND   \n",
       "1         BRICK KILN CLOSE     COGGESHALL     COLCHESTER       BRAINTREE   \n",
       "2         RAINSBROOK DRIVE        SHIRLEY       SOLIHULL        SOLIHULL   \n",
       "3               MERRY HILL  BRIERLEY HILL  BRIERLEY HILL          DUDLEY   \n",
       "4            DENMAN STREET      ROTHERHAM      ROTHERHAM       ROTHERHAM   \n",
       "...                    ...            ...            ...             ...   \n",
       "1004826           FOXLEASE        BEDFORD        BEDFORD         BEDFORD   \n",
       "1004827  QUEENSLAND AVENUE         LONDON         LONDON          MERTON   \n",
       "1004828     CONYNGHAM ROAD     MANCHESTER     MANCHESTER      MANCHESTER   \n",
       "1004829      BRAMBLE CLOSE      HAVERHILL      HAVERHILL  ST EDMUNDSBURY   \n",
       "1004830                NaN            NaN            NaN             NaN   \n",
       "\n",
       "            MILTON KEYNES.2 PPD Category Type  \\\n",
       "0             TYNE AND WEAR                 A   \n",
       "1                     ESSEX                 A   \n",
       "2             WEST MIDLANDS                 A   \n",
       "3             WEST MIDLANDS                 A   \n",
       "4           SOUTH YORKSHIRE                 A   \n",
       "...                     ...               ...   \n",
       "1004826        BEDFORDSHIRE                 A   \n",
       "1004827      GREATER LONDON                 A   \n",
       "1004828  GREATER MANCHESTER                 A   \n",
       "1004829             SUFFOLK                 A   \n",
       "1004830                 NaN               NaN   \n",
       "\n",
       "        Record Status - monthly file only  \n",
       "0                                       A  \n",
       "1                                       A  \n",
       "2                                       A  \n",
       "3                                       A  \n",
       "4                                       A  \n",
       "...                                   ...  \n",
       "1004826                                 A  \n",
       "1004827                                 A  \n",
       "1004828                                 A  \n",
       "1004829                                 A  \n",
       "1004830                               NaN  \n",
       "\n",
       "[1004831 rows x 16 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "South_east = ['OXFORDSHIRE','BUCKINGHAMSHIRE','BERKSHIRE','SURREY','KENT','EAST SUSSEX','WEST SUSSEX','ISLE OF WIGHT','HAMPSHIRE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'County'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.pyenv/versions/3.10.10/lib/python3.10/site-packages/pandas/core/indexes/base.py:3803\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3803\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.10/lib/python3.10/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.10/lib/python3.10/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'County'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Dateset containing South East England only\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCounty\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39misin([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOXFORDSHIRE\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBUCKINGHAMSHIRE\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBERKSHIRE\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSURREY\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKENT\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEAST SUSSEX\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWEST SUSSEX\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mISLE OF WIGHT\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHAMPSHIRE\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      3\u001b[0m df2 \u001b[38;5;241m=\u001b[39m df1\u001b[38;5;241m.\u001b[39mloc[mask]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.10/lib/python3.10/site-packages/pandas/core/frame.py:3805\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3804\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3805\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3807\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.10/lib/python3.10/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3803\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3810\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'County'"
     ]
    }
   ],
   "source": [
    "# Dateset containing South East England only\n",
    "mask = df['County'].isin(['OXFORDSHIRE','BUCKINGHAMSHIRE','BERKSHIRE','SURREY','KENT','EAST SUSSEX','WEST SUSSEX','ISLE OF WIGHT','HAMPSHIRE'])\n",
    "df2 = df1.loc[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Skewness : %f\" % df2['Price'].skew() )\n",
    "print(\"Kurtosis : %f\" % df2['Price'].kurt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2['County'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "county_counts = df2['County'].value_counts()\n",
    "\n",
    "# Calculate the percentage of each county\n",
    "county_percentages = county_counts / len(df2) * 100\n",
    "\n",
    "# Create a new DataFrame with the results\n",
    "county_df = pd.DataFrame({\n",
    "    'County': county_percentages.index,\n",
    "    'Percentage of dataframe': county_percentages.values\n",
    "})\n",
    "\n",
    "# Print the new DataFrame\n",
    "county_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_df1 = df2.sort_values(by='Date of Transfer', ascending=False)\n",
    "\n",
    "# Filter the DataFrame by county\n",
    "county_df = county_df1[county_df1['County'] == 'BERKSHIRE']\n",
    "\n",
    "# Select the latest data entry for the county\n",
    "latest_entry = county_df.iloc[0]\n",
    "\n",
    "latest_entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the latest data entry for Berkshire in the dataset was in 2004. For this reason it would be unreliable to continue including this in the data for prediciting housing prices using 'out of date' data as this would perform poorly on future house predictions. \n",
    "\n",
    "Therefore we will drop this county from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2[df2['County'] != 'BERKSHIRE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check that Berkshire has been removed\n",
    "df2['County'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x = df2['Price'] , y = df2['Property Type'] ,hue =df2['Property Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = df2.isnull().sum().sort_values(ascending=False)\n",
    "percent = (df2.isnull().sum()/df2.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "Dataframe_missing = pd.DataFrame(missing_data)\n",
    "Dataframe_missing.style.set_caption(\"Complete dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Drop irrelevant columns\n",
    "df3 = df2.drop(['SAON','Locality'],axis = 1)\n",
    "\n",
    "# Drop rest of missing values\n",
    "df3 = df3.dropna(subset = ['Street', 'PAON','Postcode'] , axis = 0)\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_02 = df3.isnull().sum().sort_values(ascending=False)\n",
    "percent_02= (df3.isnull().sum()/df3.isnull().count()).sort_values(ascending=False)\n",
    "missing_data_02 = pd.concat([total_02, percent_02], axis=1, keys=['Total', 'Percent'])\n",
    "Dataframe_missing_02 = pd.DataFrame(missing_data_02)\n",
    "Dataframe_missing_02.style.set_caption(\"Complete dataset removed NA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These values can be adjusted to the housing agents desire.\n",
    "plt.figure(figsize = (20,10))\n",
    "lq = df2[\"Price\"].quantile(0.01)\n",
    "uq = df2[\"Price\"].quantile(0.99)\n",
    "# print(lq)\n",
    "# print(uq)\n",
    "df_filtered = df2[(df2[\"Price\"] < uq) & (df2[\"Price\"] > lq)]\n",
    "graph = sns.histplot(df_filtered, bins = 30,kde = True);\n",
    "graph.set_title(\"Price distribution\");\n",
    "graph.set_xlabel('Price');\n",
    "graph.set_ylabel('Freqency');\n",
    "graph.ticklabel_format(useOffset=False);\n",
    "plt.ticklabel_format(style='plain') # removes scientific notation\n",
    "plt.xticks(rotation=45);\n",
    "\n",
    "# The graph below showing a 99% distribution confidence interval of house prices.\n",
    "# kernal density estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = stats.probplot(df3['Price'], plot=plt)\n",
    "\n",
    "# This shows that that house prices are not normally distributed.\n",
    "# Right skewed data (positive skew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.boxplot(x = df2['Price'] , y = df2['Property Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering out the outliers based on property type column\n",
    "# Method: removing lower 1% of data and 1.5*IQR + q3 for upper tail.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Outliers removed for property type D\n",
    "filt_D = df2['Property Type'] == 'D'\n",
    "df_filtD = df2.loc[filt_D]\n",
    "q1 = df_filtD[\"Price\"].quantile(0.25)\n",
    "q3 = df_filtD[\"Price\"].quantile(0.75)\n",
    "lq = df_filtD[\"Price\"].quantile(0.01) # $30,000 and below \n",
    "iqr = q3 - q1\n",
    "df_filt1_priceD = df_filtD[(df_filtD[\"Price\"] < q3 + 1.5*iqr) & (df_filtD[\"Price\"] > lq)]\n",
    "df_filt1_priceD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outliers removed for property type F\n",
    "filt_F = df2['Property Type'] == 'F'\n",
    "df_filtF = df2.loc[filt_F]\n",
    "q1 = df_filtF[\"Price\"].quantile(0.25)\n",
    "q3 = df_filtF[\"Price\"].quantile(0.75)\n",
    "lq = df_filtF[\"Price\"].quantile(0.01)\n",
    "iqr = q3 - q1\n",
    "df_filt1_priceF = df_filtF[(df_filtF[\"Price\"] < q3 + 1.5*iqr) & (df_filtF[\"Price\"] > lq)]\n",
    "df_filt1_priceF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outliers removed for property type O\n",
    "filt_O = df2['Property Type'] == 'O'\n",
    "df_filtO = df2.loc[filt_O]\n",
    "q1 = df_filtO[\"Price\"].quantile(0.25)\n",
    "q3 = df_filtO[\"Price\"].quantile(0.75)\n",
    "lq = df_filtO[\"Price\"].quantile(0.01)\n",
    "iqr = q3 - q1\n",
    "df_filt1_priceO = df_filtO[(df_filtO[\"Price\"] < q3 + 1.5*iqr) & (df_filtO[\"Price\"] > lq)]\n",
    "df_filt1_priceO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outliers removed for property type T\n",
    "filt_T = df2['Property Type'] == 'T'\n",
    "df_filtT = df2.loc[filt_T]\n",
    "q1 = df_filtT[\"Price\"].quantile(0.25)\n",
    "q3 = df_filtT[\"Price\"].quantile(0.75)\n",
    "lq = df_filtT[\"Price\"].quantile(0.01)\n",
    "iqr = q3 - q1\n",
    "df_filt1_priceT = df_filtT[(df_filtT[\"Price\"] < q3 + 1.5*iqr) & (df_filtT[\"Price\"] > lq)]\n",
    "df_filt1_priceT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Outliers removed for property type S\n",
    "filt_S = df2['Property Type'] == 'S'\n",
    "df_filtS = df2.loc[filt_S]\n",
    "q1 = df_filtS[\"Price\"].quantile(0.25)\n",
    "q3 = df_filtS[\"Price\"].quantile(0.75)\n",
    "lq = df_filtS[\"Price\"].quantile(0.01)\n",
    "iqr = q3 - q1\n",
    "df_filt1_priceS = df_filtS[(df_filtS[\"Price\"] < q3 + 1.5*iqr) & (df_filtS[\"Price\"] > lq)]\n",
    "df_filt1_priceS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filtered_df_property_outliers = pd.concat([df_filt1_priceD, df_filt1_priceF,df_filt1_priceO,df_filt1_priceS,df_filt1_priceT], axis=0)\n",
    "filtered_df_property_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_property_outliers['Price'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x = 'P')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_property_outliers[\"Year\"] = pd.DatetimeIndex(filtered_df_property_outliers[\"Date of Transfer\"]).year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get the first part of the postcode\n",
    "filtered_df_property_outliers['Postcode_01'] = filtered_df_property_outliers['Postcode'].str[:-3]\n",
    "filtered_df_property_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_property_outliers['Postcode_01'] = filtered_df_property_outliers['Postcode_01'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_property_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "value_counts = filtered_df_property_outliers['Postcode_01'].value_counts()\n",
    "value_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_property_outliers_copy = filtered_df_property_outliers.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filtered_df_property_outliers_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_property_outliers_copy['Price'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_county = df3.groupby(\"County\").agg([\"mean\"]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_county.columns = [\"County\", \"Avg_Price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_county\n",
    "\n",
    "Dataframe_county = pd.DataFrame(df_county)\n",
    "Dataframe_county.style.set_caption(\"Average price by County\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,20))\n",
    "plt.figure(figsize=(30,20))\n",
    "b = sns.barplot(data = df_county, x = \"County\", y = \"Avg_Price\",order= df_county.sort_values('Avg_Price',ascending = False).County);\n",
    "b.axes.set_title(\"Average price by county\",fontsize=50)\n",
    "b.set_xlabel(\"County\",fontsize=30)\n",
    "b.set_ylabel(\"Average price\",fontsize=30)\n",
    "b.tick_params(labelsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sns.set_style(\"white\")\n",
    "sns.set_color_codes(palette=\"deep\")\n",
    "f,ax = plt.subplots(figsize=(14,10))\n",
    "\n",
    "sns.distplot(df2['Price'], color=\"Blue\" )\n",
    "ax.set(ylabel = \"Frequency\")\n",
    "ax.set(xlabel = \"SalePrice\")\n",
    "ax.set(title = \"SalePrice Distribution\" )\n",
    "sns.despine(trim=True, left=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Skewness : %f\" % df2['Price'].skew() )\n",
    "print(\"Kurtosis : %f\" % df2['Price'].kurt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['Date of Transfer'] = pd.to_datetime(df3['Date of Transfer'])\n",
    "# Ensure that this column is a date-time object so that we can use this later for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3[\"Year\"] = pd.DatetimeIndex(df3[\"Date of Transfer\"]).year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_year = df3.groupby(\"Year\").agg([\"mean\"]).reset_index()\n",
    "dataframe_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_year.columns = [\"Year\", \"Avg_Price\"]\n",
    "dataframe_year.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plot01 = sns.barplot(data = dataframe_year, x = \"Year\", y = \"Avg_Price\")\n",
    "plot01.axes.set_title(\"Average price by year\",fontsize=30)\n",
    "plot01.tick_params(labelsize= 12)\n",
    "plot01.set_xlabel(\"Year\",fontsize=20)\n",
    "plot01.set_ylabel(\"Average price\",fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "plot02 = sns.countplot(x='Property Type', data=df3,order = df3['Property Type'].value_counts().index)\n",
    "plot02.axes.set_title(\"Count plot of Property Type\",fontsize=20)\n",
    "plot02.set_xlabel(\"Property Type\",fontsize=15)\n",
    "plot02.set_ylabel(\"Frequency\",fontsize=15)\n",
    "plot02.tick_params(labelsize=15)\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "plot03 = sns.countplot(x='County', data=df3,order = df3['County'].value_counts().index)\n",
    "plot03.axes.set_title(\"Count plot of county\",fontsize=20)\n",
    "plot03.set_xlabel(\"County\",fontsize=15)\n",
    "plot03.set_ylabel(\"Frequency\",fontsize=15)\n",
    "plot03.tick_params(labelsize=15)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "df2['Date of Transfer'] = pd.to_datetime(df2['Date of Transfer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['month'] = df2['Date of Transfer'].dt.month\n",
    "df2['year'] = df2['Date of Transfer'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as mtick\n",
    "\n",
    "grouped_df = df2.groupby(['County', 'year', 'month'])['Price'].mean().reset_index()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "\n",
    "for county in grouped_df['County'].unique():\n",
    "    county_data = grouped_df[grouped_df['County'] == county]\n",
    "    ax.plot(county_data['year'] + (county_data['month'] - 1) / 12, county_data['Price'], label=county)\n",
    "\n",
    "ax.legend()\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Average House Price')\n",
    "ax.set_title('Monthly Average House Prices')\n",
    "\n",
    "# Format y-axis tick labels as regular numbers\n",
    "fmt = '£{x:,.0f}'\n",
    "tick = mtick.StrMethodFormatter(fmt)\n",
    "ax.yaxis.set_major_formatter(tick)\n",
    "ax.legend(bbox_to_anchor=(0.5, 1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df2['Price'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lq = df2[\"Price\"].quantile(0.01)\n",
    "uq = df2[\"Price\"].quantile(0.99)\n",
    "# df_filtered03 = df2[(df2[\"Price\"] < 607500) & (df2[\"Price\"] > lq)]\n",
    "df_filtered03 = df2[(df2[\"Price\"] < 607500) & (df2[\"Price\"] > lq)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lq)\n",
    "print(uq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_filtered03['Price'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_filtered03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.boxplot(data = df2 , x = 'Price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(data = df2, x = np.log(df2['Price']))\n",
    "ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate summary statistics\n",
    "print(\"Summary Statistics:\")\n",
    "print(\"Mean: {:.2f}\".format(np.mean(np.log(df2[\"Price\"]))))\n",
    "print(\"Median: {:.2f}\".format(np.median(np.log(df2[\"Price\"]))))\n",
    "print(\"Minimum: {:.2f}\".format(np.min(np.log(df2[\"Price\"]))))\n",
    "print(\"Maximum: {:.2f}\".format(np.max(np.log(df2[\"Price\"]))))\n",
    "print(\"25th percentile: {:.2f}\".format(np.percentile(np.log(df2[\"Price\"]), 25)))\n",
    "print(\"75th percentile: {:.2f}\".format(np.percentile(np.log(df2[\"Price\"]), 75)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower = 10.72\n",
    "higher = 13.66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_transformed = df2.copy()\n",
    "df_transformed['Price'] = np.log(df2['Price'])\n",
    "df_transformed_mean = df_transformed['Price'].mean()\n",
    "df_transformed_median = df_transformed['Price'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_outliers_below = df_transformed.loc[df_transformed['Price'] < lower]\n",
    "price_outliers_abow = df_transformed.loc[df_transformed['Price'] > higher]\n",
    "\n",
    "print(price_outliers_below['Price'].count(), \"entries having 'Price' value lower than \", lower)\n",
    "print(price_outliers_abow['Price'].count(), \"entries having 'Price' value greater than\", higher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total data removed  = 219,708 / 3681940 * 100 = 5.97%\n",
    "\n",
    "df_transformed[\"Price\"] = df_transformed[\"Price\"].apply(lambda x: lower if x < lower else x)\n",
    "df_transformed[\"Price\"] = df_transformed[\"Price\"].fillna(df_transformed_median)\n",
    "\n",
    "df_transformed[\"Price\"] = df_transformed[\"Price\"].apply(lambda x: higher if x > higher else x)\n",
    "df_transformed[\"Price\"] = df_transformed[\"Price\"].fillna(df_transformed_median)\n",
    "\n",
    "price_outliers_below = df_transformed.loc[df_transformed['Price'] < lower]\n",
    "price_outliers_above = df_transformed.loc[df_transformed['Price'] > higher]\n",
    "\n",
    "print(price_outliers_below['Price'].count(), \"entries having 'Price' value lower than \", lower)\n",
    "print(price_outliers_above['Price'].count(), \"entries having 'Price' value greater than \", higher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.boxplot(data = df_transformed, x = 'Price');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_transformed = df_transformed.drop(['Locality','PPD Category Type','Record Status - monthly file only','Street','SAON','PAON','Postcode'] ,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_transformed\n",
    "\n",
    "# NEED THIS SECTION FOR CREATING A NEW DATAFRAME WITH OUTLIERS REMOVED."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed_model = df_transformed.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed['Town/City'] = df_transformed ['Town/City'].factorize()[0].astype('float32')\n",
    "df_transformed['Old/New'] = df_transformed ['Old/New'].factorize()[0].astype('float32')\n",
    "df_transformed['County'] = df_transformed ['County'].factorize()[0].astype('float32')\n",
    "df_transformed['Duration'] = df_transformed ['Duration'].factorize()[0].astype('float32')\n",
    "df_transformed['District'] = df_transformed ['District'].factorize()[0].astype('float32')\n",
    "df_encoded1 = pd.get_dummies(df_transformed, columns = [\"Property Type\"], prefix = [\"Property_Type_is_\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(df_encoded1[df_encoded1.corr().index].corr(), annot = True, cmap = 'RdYlGn')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the heatmap above reveals that year is strongly correlated to the housing price which comes to no surprise. There also seems to be some correlation between duration and property type with respect to price. Also notice from the table below that old/new of the house, that median value is very similar and suggest weak correlation to price. Also supported by the heatmap showing close to 0 correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filtered_df_property_outliers.groupby('Old/New')['Price'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mutual Information score\n",
    "\n",
    "To decide which variables to include in our model, mutual information score was used to assess whether a feature was plausible to be included in a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df2.copy().drop('Date of Transfer' , axis = 1)[:100000]\n",
    "y = X.pop(\"Price\")[:100000]\n",
    "\n",
    "# Label encoding for categoricals\n",
    "for colname in X.select_dtypes(\"object\"):\n",
    "    X[colname], _ = X[colname].factorize()\n",
    "\n",
    "# All discrete features should now have integer dtypes (double-check this before using MI!)\n",
    "discrete_features = X.dtypes == int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "discrete_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "def make_mi_scores(X, y, discrete_features):\n",
    "    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n",
    "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "    return mi_scores\n",
    "\n",
    "mi_scores = make_mi_scores(X, y, discrete_features)\n",
    "mi_scores  # show a few features with their MI scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "def plot_mi_scores(scores):\n",
    "    scores = scores.sort_values(ascending=True)\n",
    "    width = np.arange(len(scores))\n",
    "    ticks = list(scores.index)\n",
    "    plt.barh(width, scores)\n",
    "    plt.yticks(width, ticks)\n",
    "    plt.title(\"Mutual Information Scores\")\n",
    "    plt.grid()\n",
    "\n",
    "plt.figure(dpi=100, figsize=(8, 5))\n",
    "plot_mi_scores(mi_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that location of the house indicates a large reduction in uncertainty in the housing price. Now since postcode, street and locality are all related to each other, we have decided to just include postcode as our region of reference for the property. We also filtered out redundant features such as 'Old/New' and 'County', which have low MI scores and are uninformative in explaining variance in housing prices. \n",
    "\n",
    "Our model will include the features: 'Property Type', 'Duration', 'District', 'Year', and 'Postcode'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_property_outliers_copy['District'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_property_outliers_copy['Postcode_01'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Postcode'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filtered_df_property_outliers_copy['Postcode_01'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prevent overfitting, we dropped postcodes that were recorded less than 100 times, leaving us with 347 postcodes for our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 100\n",
    "\n",
    "# Filter out Postcode_01 based on frequency threshold\n",
    "counts = filtered_df_property_outliers_copy['Postcode_01'].value_counts()\n",
    "keep = counts[counts >= threshold].index\n",
    "filtered_df_property_outliers_copy = filtered_df_property_outliers_copy.loc[filtered_df_property_outliers_copy['Postcode_01'].isin(keep)]\n",
    "filtered_df_property_outliers_copy['Postcode_01'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filtered_df_property_outliers_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import make_column_transformer \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import make_pipeline \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filtered_df_property_outliers_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL 1 \n",
    "\n",
    "X = filtered_df_property_outliers_copy[['Property Type','Duration','District','Year','Postcode_01']]\n",
    "y = filtered_df_property_outliers_copy[['Price']] # target variable\n",
    "\n",
    "column_trans = make_column_transformer(\n",
    "                (OneHotEncoder(handle_unknown = 'ignore'), ['Duration','District','Postcode_01']),\n",
    "                (OrdinalEncoder(), ['Property Type','Year']),\n",
    "                remainder = 'drop')\n",
    "\n",
    "# We set handle_unknown='ignore' to avoid errors when the validation data contains\n",
    "# classes that aren't represented in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decicion = DecisionTreeRegressor()\n",
    "random = RandomForestRegressor()\n",
    "linear = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct some pipelines\n",
    "\n",
    "pipe_lr = make_pipeline(column_trans,linear)\n",
    "pipe_rf = make_pipeline(column_trans,random)\n",
    "pipe_dt = make_pipeline(column_trans,decicion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# DECISION TREE \n",
    "\n",
    "pipe_dt.fit(X_train,y_train.values.ravel())\n",
    "predictions_dt = pipe_dt.predict(X_test)\n",
    "print(sqrt(mean_squared_error(y_test, predictions_dt))) # Root mean square error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LINEAR REGRESSION \n",
    "pipe_lr.fit(X_train,y_train.values.ravel())\n",
    "predictions_lr = pipe_lr.predict(X_test)\n",
    "print(sqrt(mean_squared_error((y_test),(predictions_lr))))\n",
    "\n",
    "\n",
    "# The regression has five key assumptions:\n",
    "# Linear relationship.\n",
    "# Multivariate normality.\n",
    "# No or little multicollinearity.\n",
    "# No auto-correlation.\n",
    "# Homoscedasticity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R-squared value of the linear model\n",
    "r2_score(y_test, predictions_lr, force_finite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "fig = plt.figure(figsize = (20,10))\n",
    "\n",
    "# stats.probplot(predictions_lr)\n",
    "\n",
    "residuals = predictions_lr - y_test.to_numpy().reshape(len(y_test),)\n",
    "\n",
    "sm.qqplot(residuals,fit=True,line='45',dist=stats.norm)\n",
    "\n",
    "plt.title(\"No transformation\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# logged price\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize = (20,10))\n",
    "\n",
    "# stats.probplot(predictions_lr)\n",
    "\n",
    "risiduals = predictions_lr - y_test.to_numpy().reshape(len(y_test),)\n",
    "\n",
    "sm.qqplot(risiduals,fit=True,line='45',dist=stats.norm)\n",
    "\n",
    "plt.title(\"log transformation\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We logged the response variable price but the plot reveals the residuals are still far from being normal. This violates the assumption that the residuals are normally distributed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Lets see a histogram of the residuals\n",
    "\n",
    "residuals = predictions_lr[:100000] - y_test.to_numpy()[:100000].reshape(100000,)\n",
    "plt.hist(residuals, bins=30)\n",
    "plt.title('Histogram of Residuals')\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2 , figsize=(16, 6))\n",
    "\n",
    "residuals = predictions_lr[:500] - y_test.to_numpy()[:500].reshape(500,)\n",
    "\n",
    "# Lets see a histogram of the residuals\n",
    "ax1.hist(residuals)\n",
    "ax1.set_xlabel('Residuals')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Histogram of Residuals')\n",
    "\n",
    "# Plot the residuals against the fitted values\n",
    "ax2.scatter(predictions_lr[:500], residuals)\n",
    "ax2.set_xlabel('Fitted values')\n",
    "ax2.set_ylabel('Residuals')\n",
    "ax2.set_title('Residuals against the fitted values')\n",
    "ax2.axhline(y=0, linestyle='--', color='r')\n",
    "\n",
    "# Calculate the slope and intercept of the line of best fit\n",
    "slope, intercept = np.polyfit(predictions_lr[:500], residuals, 1)\n",
    "\n",
    "# Plot the line of best fit\n",
    "plt.plot(predictions_lr[:500], slope * predictions_lr[:500] + intercept, color='g')\n",
    "\n",
    "\n",
    "fig.set_size_inches(20, 6)\n",
    "\n",
    "# Display the figure\n",
    "plt.subplots_adjust(wspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above figure shows a residual plot of the residuals against the fitted values. The purpose of the plot is to check if the assumption of heteroscedasticity is satisfied in the linear model. Clearly it shows a ‘right-opening megaphone’ residuals plot implying that the variance is not constant between observations. We can observe that the vertical scatter increases as the response variable (price) increases. The same conclusion is reached on the plot of the left. The violation of this assumption also means that the errors are not normally distributed. To address this issue, we can attempt to do a log transformation of the response variable to stabilise the variance and achieve heteroscedasticity.\n",
    "\n",
    "Also, a line of best fit is fitted onto the residuals plot (shown in green) in figure x. The good news is that the mean of the residuals is 0 implying that the model is not consistently overestimating or underestimating the true value.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope, intercept, r_value, p_value, std_err = stats.linregress(predictions_lr[:500], y_test.to_numpy()[:500].reshape(500,))\n",
    "\n",
    "# Create a scatter plot of the data\n",
    "plt.scatter(predictions_lr[:500],y_test[:500])\n",
    "\n",
    "# Add the regression line to the plot\n",
    "plt.plot(predictions_lr[:500], intercept + slope*predictions_lr[:500], 'r', label='Regression Line')\n",
    "\n",
    "# Add labels and a legend to the plot\n",
    "plt.ylabel('Predictions')\n",
    "plt.xlabel('Actual')\n",
    "plt.title('Predicted vs actual')\n",
    "\n",
    "# Set x-axis format to plain decimal notation\n",
    "plt.ticklabel_format(useOffset=False, style='plain')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning for Decison tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to a coding error that could not be solved, i have had change 'Property Type' to be one-hot encoded instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# MODEL\n",
    "\n",
    "X = filtered_df_property_outliers_copy[['Property Type','Duration','District','Year','Postcode_01']]\n",
    "y = filtered_df_property_outliers_copy[['Price']] # target variable\n",
    "\n",
    "column_trans = make_column_transformer(\n",
    "                (OneHotEncoder(handle_unknown = 'ignore'), ['Duration','District','Property Type','Postcode_01']),\n",
    "                (OrdinalEncoder(), ['Year']),\n",
    "                remainder = 'drop')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "decicion = DecisionTreeRegressor()\n",
    "\n",
    "pipe_dt = make_pipeline(column_trans,decicion)\n",
    "\n",
    "\n",
    "# I HAVE MOVED PROPERTY TYPE FROM ORDINAL ENCODING TO ONEHOTENCODING TO DEAL WITH MISSING VALUES IN THE TEST SET.\n",
    "\n",
    "# Get training and test scores, see chatgbt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# AFTER HYPERPARAMTER TUNING\n",
    "pipe_dt.fit(X_train,y_train.values.ravel())\n",
    "predictions_dt = pipe_dt.predict(X_test)\n",
    "print(sqrt(mean_squared_error(y_test, predictions_dt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R-squared value of the decison model (after tuning)\n",
    "r2_score(y_test, predictions_dt, force_finite=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEFORE\n",
    "pipe_dt.fit(X_train,y_train.values.ravel())\n",
    "predictions_dt = pipe_dt.predict(X_test)\n",
    "print(sqrt(mean_squared_error(y_test, predictions_dt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R-squared value of the decison model (before tuning)\n",
    "r2_score(y_test, predictions_dt, force_finite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# BEFORE HYPERPARAMETER TUNIN\n",
    "# Compute the training accuracy\n",
    "training_accuracy = pipe_dt.score(X_train, y_train)\n",
    "print('Training accuracy:', training_accuracy)\n",
    "\n",
    "# Compute the testing accuracy\n",
    "testing_accuracy = pipe_dt.score(X_test, y_test)\n",
    "print('Testing accuracy:', testing_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AFTER HYPERPARAMTER TUNING\n",
    "\n",
    "# Compute the training accuracy\n",
    "training_accuracy = pipe_dt.score(X_train, y_train)\n",
    "print('Training accuracy:', training_accuracy)\n",
    "\n",
    "# Compute the testing accuracy\n",
    "testing_accuracy = pipe_dt.score(X_test, y_test)\n",
    "print('Testing accuracy:', testing_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see that after hyperparameter tuning and implementing the best parameters into the model, the training and testing accuracy have actually fallen from the default parameters. This is somewhat expected since there are billions of other possible combinations and only considering two hyperparameters to tune in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the paper “Theoretical comparison between the Gini Index and Information Gain criteria” [1], the choice of the splitting criteria does not make much of a difference in the tree's performance. For this reason, the default method used, which is the “gini” impurity measure will be left untouched. \n",
    "\n",
    "[1] Laura Elena Raileanu and Kilian Stoffel, “Theoretical comparison between the Gini Index and Information Gain criteria” Annals of Mathematics and Artificial Intelligence 41: 77–93, 2004.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Decision tree \n",
    "import numpy as np \n",
    "\n",
    "max_depth = [3,5,7,9,11,13,15,17,19]\n",
    "\n",
    "max_leaf_nodes = [60,70,80,90,100]\n",
    "\n",
    "parameters = dict(\n",
    "                decisiontreeregressor__max_leaf_nodes=max_leaf_nodes,\n",
    "                decisiontreeregressor__max_depth=max_depth)\n",
    "\n",
    "grid_search = GridSearchCV(pipe_dt, parameters, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "grid_search.fit(X,y)\n",
    "\n",
    "grid_search.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code evaluates all 9x5 = 45 combinations of max_depth and max_leaf_nodes and it will train each model five times since we are using a five-fold cross validation. In other words, there will be 45x5 rounds of training! \n",
    "\n",
    "The search becomes computationally costly as the number of combinations increase. For very large search space, we recommend using RandomizedSearchCV which instead of trying out all combinations, it evaluates a given number of random combinations by selecting a random value for each hyperparameter at every iteration (page 81)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipe_dt.get_params().keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
